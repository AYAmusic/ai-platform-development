# Playwright MCP + Langbase Testing Workflow

## WORKFLOW DESIGN

The Playwright MCP + Langbase Testing Workflow automates the end-to-end testing of the AI platform, focusing on both UI interactions via Playwright Model Context Protocol (MCP) and backend AI logic validation through Langbase. This workflow is designed to provide comprehensive testing, identify issues early, and ensure the quality and reliability of the hybrid AI system.

**Input**: Test scenario specifications (e.g., user stories, feature descriptions, specific UI flows).
**Process**:
1.  **Test Scenario Parsing**: Interpret test scenario specifications into actionable test steps.
2.  **Hybrid Test Execution**: Orchestrate UI interactions using Playwright MCP and validate AI responses/backend logic using Langbase Pipes or direct API calls.
3.  **Visual and Functional Validation**: Perform visual diff analysis for UI changes and functional assertions for AI outputs.
4.  **Error Reporting**: Capture detailed error logs, screenshots, and performance metrics for failed tests.
5.  **Reporting and Analysis**: Generate comprehensive test reports for human review.
**Output**: Test results (pass/fail), detailed logs, screenshots, visual diffs, and analytical reports.

## COMPONENTS

### 1. Test Scenario Interpreter
**Responsibility**: Converts high-level test descriptions into a structured sequence of executable test actions.
**Logic**: 
*   Parses natural language or structured test definitions (e.g., Gherkin syntax, JSON objects).
*   Maps test steps to corresponding Playwright actions (click, type, navigate) and Langbase interactions (run Pipe, query Memory).

### 2. Playwright MCP Executor
**Responsibility**: Drives UI interactions and captures visual states using Playwright.
**Logic**: 
*   Launches and controls browser instances via Playwright.
*   Executes UI actions (clicks, form inputs, navigations) based on interpreted test steps.
*   Captures screenshots for visual diff analysis at predefined checkpoints.
*   Interacts with the UI assistant via the Model Context Protocol to feed browser context and receive instructions.

### 3. Langbase Test Validator
**Responsibility**: Validates the backend AI logic and responses generated by Langbase.
**Logic**: 
*   Invokes specific Langbase Pipes or directly interacts with Langbase Memory/APIs.
*   Asserts the correctness, completeness, and quality of AI-generated content against expected outputs.
*   Monitors Langbase API usage and performance during testing.

### 4. Visual Diff Analyzer
**Responsibility**: Compares current UI screenshots against baseline images to detect unintended visual changes.
**Logic**: 
*   Stores baseline screenshots for each test scenario.
*   Compares new screenshots with baselines, highlighting pixel-level differences.
*   Quantifies the extent of visual changes and flags significant deviations as potential issues.

### 5. Test Reporter & Analyzer
**Responsibility**: Aggregates test results, generates reports, and provides analytical insights.
**Logic**: 
*   Collects pass/fail status from all test steps.
*   Includes detailed error messages, stack traces, and relevant context (e.g., network logs, console output).
*   Embeds screenshots and visual diff reports.
*   Generates comprehensive test reports in `/dev_assistant_reports/` and potentially `dev_assistant_screenshots/`.
*   Analyzes trends in test failures and performance regressions.

## IMPLEMENTATION

The Playwright MCP + Langbase Testing Workflow will be a Python-based implementation, likely residing in the `tests/` directory, leveraging Playwright's Python library and direct HTTP calls to Langbase APIs. The `dev_assistant_monitor.py` and `ui_integrated_assistant.py` could serve as foundational elements.

```python
# tests/e2e/test_content_generation.py

import asyncio
from playwright.async_api import Playwright, async_playwright, expect
import requests # For Langbase API calls
import os

# Placeholder for Playwright MCP integration (assuming a custom client)
class PlaywrightMCPClient:
    def __init__(self, page):
        self.page = page

    async def send_context_to_model(self, context_data):
        # Simulate sending browser context to an AI model for analysis/instructions
        print(f"Sending context to model: {context_data['type']}")
        # In a real scenario, this would involve a WebSocket or HTTP POST to an AI agent
        await asyncio.sleep(0.1)
        return {"instruction": "proceed"} # Example instruction

    async def receive_instructions_from_model(self):
        # Simulate receiving instructions from an AI model
        await asyncio.sleep(0.1)
        return {"action": "click", "selector": "#submit-button"} # Example instruction

# Placeholder for Langbase Client
class LangbaseTestClient:
    def __init__(self, api_key: str, base_url: str = "https://api.langbase.com"):
        self.api_key = api_key
        self.base_url = base_url

    def _headers(self):
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def run_pipe(self, pipe_id: str, data: dict):
        url = f"{self.base_url}/pipes/{pipe_id}/run"
        response = requests.post(url, headers=self._headers(), json=data)
        response.raise_for_status()
        return response.json()

    def query_memory(self, memory_id: str, query: str):
        url = f"{self.base_url}/memory/{memory_id}/query"
        response = requests.post(url, headers=self._headers(), json={"query": query})
        response.raise_for_status()
        return response.json()

async def run_test_scenario(playwright: Playwright, scenario_name: str):
    browser = await playwright.chromium.launch(headless=True)
    page = await browser.new_page()
    mcp_client = PlaywrightMCPClient(page)
    langbase_client = LangbaseTestClient(api_key=os.getenv("LANGBASE_API_KEY", "YOUR_LANGBASE_API_KEY"))

    try:
        print(f"\nRunning scenario: {scenario_name}")
        await page.goto("http://localhost:3000") # Assuming Open WebUI runs on port 3000
        await page.screenshot(path=f"dev_assistant_screenshots/{scenario_name}_start.png")

        # --- Test Scenario Steps ---

        # Step 1: Simulate user inputting a prompt into Open WebUI
        await page.fill("#prompt-input", "Generate a report on AI workflow optimization.")
        await mcp_client.send_context_to_model({"type": "input_filled", "element_id": "prompt-input", "value": "Generate a report on AI workflow optimization."})
        await page.click("#send-button")
        print("Simulated prompt input and send.")

        # Step 2: Verify a loading indicator or initial response (UI)
        await expect(page.locator("#response-area")).not_to_be_empty()
        await page.screenshot(path=f"dev_assistant_screenshots/{scenario_name}_after_send.png")

        # Step 3: Validate AI response from Langbase (backend validation)
        # This would typically involve checking a specific API endpoint or a mock.
        # For demonstration, simulate calling a Langbase Pipe to validate expected output format/content
        print("Validating AI response via Langbase Test Client...")
        langbase_validation_result = langbase_client.run_pipe(
            pipe_id="content_validation_pipe", # A Langbase Pipe designed for validation
            data={"generated_text": await page.locator("#response-area").inner_text(), "expected_keywords": ["workflow", "optimization"]}
        )
        expect(langbase_validation_result["isValid"]).to_be_true()
        print(f"Langbase validation result: {langbase_validation_result}")

        # Step 4: Perform a visual diff (conceptual, actual implementation uses external tools)
        print("Performing conceptual visual diff...")
        # In a real setup, an external visual diff tool (like a Percy integration) would compare screenshots

        print(f"Scenario '{scenario_name}' completed successfully.")

    except Exception as e:
        print(f"Scenario '{scenario_name}' failed: {e}")
        await page.screenshot(path=f"dev_assistant_screenshots/{scenario_name}_fail.png")
        raise # Re-raise to mark test as failed
    finally:
        await browser.close()

async def main():
    # Ensure the screenshots directory exists
    os.makedirs("dev_assistant_screenshots", exist_ok=True)
    
    async with async_playwright() as playwright:
        await run_test_scenario(
            playwright,
            "Content_Generation_Pipeline_End_to_End_Test"
        )

if __name__ == "__main__":
    asyncio.run(main())
```

## ERROR HANDLING

**Playwright Errors:**
*   Utilize Playwright's built-in error handling (`try-except` blocks, `expect` assertions) to catch UI-related failures (e.g., element not found, timeout).
*   Capture screenshots and HTML dumps on failure to aid debugging.

**Langbase API Errors:**
*   Implement robust error handling for Langbase API calls (e.g., network errors, authentication failures, invalid requests).
*   Use retries with exponential backoff for transient Langbase service issues.

**Visual Diff Anomalies:**
*   Define thresholds for acceptable visual differences. Minor UI rendering variations should not necessarily fail a test, but significant deviations should trigger alerts.

**Comprehensive Logging:**
*   Log all test actions, network requests, console outputs, and errors for detailed debugging.
*   Ensure logs clearly indicate the source of the error (Playwright, Langbase, or the test script itself).

## OPTIMIZATION

**Parallel Test Execution:**
*   Run multiple test scenarios concurrently using Playwright's parallel capabilities to reduce overall test suite execution time.

**Test Data Management:**
*   Use efficient test data generation and cleanup strategies to ensure tests are isolated and repeatable.
*   Parameterize tests to run with different inputs and configurations.

**Selective Test Execution:**
*   Implement tagging for tests (e.g., `@smoke`, `@regression`, `@performance`) to allow for selective execution of relevant test subsets, speeding up CI/CD cycles.

**Optimized Screenshot Capture:**
*   Capture screenshots only at critical points or on failure to minimize I/O overhead.
*   Consider using more efficient image comparison algorithms or services for visual diffing.

**Langbase Mocking/Stubbing:**
*   For faster and more isolated unit/integration tests, mock or stub Langbase API responses, especially for complex or costly Pipes.

## MONITORING

**Test Run Metrics:**
*   **Test Pass/Fail Rate**: Track the percentage of successful tests over time.
*   **Test Execution Time**: Monitor the total time taken for test suites and individual scenarios.
*   **Flaky Test Rate**: Identify tests that intermittently fail, indicating potential instability or environmental issues.
*   **Visual Diff Rate**: Track the frequency and severity of visual changes detected.

**Integration with CI/CD:**
*   Integrate test execution into the CI/CD pipeline to ensure continuous validation on every code change.
*   Publish test results to a dashboard (e.g., allure reports, Jenkins/GitHub Actions test summary) for easy visibility.

**Alerting:**
*   Set up alerts for significant drops in test pass rates, prolonged test execution times, or new critical visual regressions.
*   Alert on specific Langbase API errors or performance degradation detected during testing.

**Artifact Storage:**
*   Automatically store test artifacts (logs, screenshots, visual diff reports) in designated directories (`dev_assistant_reports/`, `dev_assistant_screenshots/`) or a centralized artifact repository for historical analysis and debugging.

**Performance Testing:**
*   Extend this workflow to include performance testing scenarios, measuring response times and resource utilization under load for both UI and backend AI components.